#!/usr/bin/env python3
"""
Production-grade(ish) RAG microservice scaffold for Oracle ADB.

✔ Works with wallet + python-oracledb (THIN mode)
✔ Connection pooling, structured logging, health checks
✔ Ingestion endpoint (with optional fake embeddings so you can test now)
✔ Retrieval endpoint (vector search when embeddings exist; keyword fallback otherwise)
✔ Answer endpoint that stitches context with a simple, deterministic stub LLM

Switch to real embeddings later by setting USE_FAKE_EMBED=0 and configuring OCI GenAI.

Run:
  pip install fastapi uvicorn[standard] python-dotenv oracledb pypdf python-docx python-pptx chardet
  export TNS_ADMIN=/home/you/wallet
  export ADB_DSN=aitutordb_tp   # or _high/_low
  export ADB_USER=ADMIN
  export ADB_PASSWORD=...
  python rag_service.py

Then visit http://127.0.0.1:8000/docs
"""
from __future__ import annotations

import os
import re
import json
import time
import hashlib
import logging
from pathlib import Path
from typing import List, Optional, Dict, Any
import numpy as np


import oracledb
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Body
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field
from dotenv import load_dotenv
#from langchain_community.embeddings.oci_generative_ai import OCIGenAIEmbeddings

# -------- logging --------
load_dotenv()
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=LOG_LEVEL,
    format="%(asctime)s %(levelname)s %(name)s - %(message)s",
)
log = logging.getLogger("rag")

# -------- env & config --------
TNS_ADMIN = os.environ.get("TNS_ADMIN")
ADB_DSN = os.environ.get("ADB_DSN")
ADB_USER = os.environ.get("ADB_USER")
ADB_PASSWORD = os.environ.get("ADB_PASSWORD")
COURSE_DEFAULT = os.environ.get("COURSE", "DATA100_FALL25")
EMBED_DIM = int(os.environ.get("EMBED_DIM", "1024"))
USE_FAKE_EMBED = os.getenv("USE_FAKE_EMBED", "0") == "1"

# OCI GenAI embedding (optional later)
OCI_REGION = os.environ.get("OCI_REGION")
COMPARTMENT_OCID = os.environ.get("COMPARTMENT_OCID")
EMBED_MODEL_ID = os.environ.get("EMBED_MODEL_ID", "cohere.embed-english-v3.0")

# -------- FastAPI --------
app = FastAPI(title="RAG Service", version="0.1.0")

# Serve static frontend assets
FRONTEND_DIR = Path(__file__).parent / "frontend"

if FRONTEND_DIR.exists():
    # /static/... for extra JS/CSS if you add them later
    app.mount(
        "/static",
        StaticFiles(directory=str(FRONTEND_DIR), html=False),
        name="static",
    )

    @app.get("/", include_in_schema=False)
    async def serve_index():
        """
        Serve the main frontend page.
        """
        index_path = FRONTEND_DIR / "index.html"
        if not index_path.exists():
            raise HTTPException(status_code=404, detail="index.html not found")
        return FileResponse(str(index_path))


# -------- DB pool --------
_pool: Optional[oracledb.ConnectionPool] = None

def get_pool() -> oracledb.ConnectionPool:
    global _pool
    if _pool is None:
        missing = [k for k, v in [("TNS_ADMIN", TNS_ADMIN), ("ADB_DSN", ADB_DSN), ("ADB_USER", ADB_USER), ("ADB_PASSWORD", ADB_PASSWORD)] if not v]
        if missing:
            raise RuntimeError(f"Missing required env: {', '.join(missing)}")
        # Thin mode pool; wallet picked up via TNS_ADMIN/sqlnet.ora
        log.info("Creating oracle pool (dsn=%s)", ADB_DSN)
        _pool = oracledb.create_pool(
            user=ADB_USER,
            password=ADB_PASSWORD,
            dsn=ADB_DSN,
            wallet_location=TNS_ADMIN,         # ✅ Add this
            wallet_password=os.environ.get("WALLET_PASSWORD"),  # ✅ Add this
            min=1,
            max=4,
            increment=1,
            timeout=60,
            wait_timeout=30,
            session_callback=_session_init,
        )
    return _pool


def _session_init(conn: oracledb.Connection, requested_tag: str) -> None:
    # Any ALTER SESSION settings go here
    with conn.cursor() as cur:
        try:
            cur.execute("ALTER SESSION SET NLS_DATE_FORMAT='YYYY-MM-DD""' HH24:MI:SS'")
        except Exception:
            pass

# -------- Schema setup --------
DDL_TABLE = f"""
CREATE TABLE doc_chunks (
  id         NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  course     VARCHAR2(64),
  source_uri VARCHAR2(1024),
  chunk_no   NUMBER,
  section    VARCHAR2(256),
  page_from  NUMBER,
  page_to    NUMBER,
  content    CLOB,
  embedding  VECTOR({EMBED_DIM}),
  metadata   JSON
)
"""
DDL_UX = """
CREATE UNIQUE INDEX doc_chunks_unique_ingest
ON doc_chunks(course, source_uri, chunk_no)
"""
DDL_IDX1 = "CREATE INDEX doc_chunks_course_idx ON doc_chunks(course)"
DDL_IDX2 = "CREATE INDEX doc_chunks_source_idx ON doc_chunks(source_uri)"
DDL_VIDX_NEW = """
CREATE VECTOR INDEX doc_chunks_vec_idx
ON doc_chunks (embedding)
PARAMETERS('ALGORITHM HNSW DISTANCE COSINE')
"""
DDL_VIDX_LEGACY = """
CREATE INDEX doc_chunks_vec_idx
ON doc_chunks (embedding)
ORGANIZATION NEIGHBOR LIST
"""


def ensure_schema(conn: oracledb.Connection) -> None:
    with conn.cursor() as cur:
        cur.execute("SELECT COUNT(*) FROM user_tables WHERE table_name='DOC_CHUNKS'")
        if cur.fetchone()[0] == 0:
            log.info("[schema] creating DOC_CHUNKS")
            cur.execute(DDL_TABLE)
            conn.commit()
        for ddl in (DDL_UX, DDL_IDX1, DDL_IDX2):
            try:
                cur.execute(ddl)
                conn.commit()
            except oracledb.Error:
                pass
        # vector index (best-effort)
        cur.execute("SELECT COUNT(*) FROM user_indexes WHERE index_name='DOC_CHUNKS_VEC_IDX'")
        if cur.fetchone()[0] == 0:
            try:
                log.info("[schema] creating vector index (new syntax)")
                cur.execute(DDL_VIDX_NEW)
                conn.commit()
            except oracledb.Error:
                try:
                    log.info("[schema] creating vector index (legacy syntax)")
                    cur.execute(DDL_VIDX_LEGACY)
                    conn.commit()
                except oracledb.Error:
                    log.warning("[schema] could not create vector index; continuing without")

# -------- Embeddings --------

def fake_embed(texts: List[str], dim: int) -> List[List[float]]:
    import numpy as np
    out: List[List[float]] = []
    for t in texts:
        seed = int(hashlib.md5(t.encode("utf-8", errors="ignore")).hexdigest(), 16) % (2**32)
        rng = np.random.RandomState(seed)
        v = rng.rand(dim)
        v = v / (float((v**2).sum()) ** 0.5 + 1e-12)
        out.append(v.tolist())
    return out

import cohere

COHERE_API_KEY = os.environ.get("COHERE_API_KEY")
if not COHERE_API_KEY:
    raise RuntimeError("COHERE_API_KEY environment variable must be set")

co = cohere.Client(COHERE_API_KEY)

EMBED_MODEL_ID = os.environ.get("EMBED_MODEL_ID", "cohere.embed-english-v3.0")

def embed(texts: List[str], is_query: bool = False) -> List[List[float]]:
    """
    Use Cohere embeddings only.
    """
    input_type = "search_query" if is_query else "search_document"
    response = co.embed(
        texts=texts,
        model=EMBED_MODEL_ID,
        input_type=input_type 
    )
    return response.embeddings


# -------- Parsing & chunking --------
from pypdf import PdfReader
from docx import Document as DocxDocument
from pptx import Presentation
import chardet

def read_txt(path: Path) -> str:
    raw = path.read_bytes()
    enc = chardet.detect(raw).get("encoding") or "utf-8"
    return raw.decode(enc, errors="ignore")

def read_pdf(path: Path) -> str:
    reader = PdfReader(str(path))
    pages = []
    for pg in reader.pages:
        try:
            t = pg.extract_text() or ""
        except Exception:
            t = ""
        pages.append(t)
    return "\n\n".join(pages)

def read_docx(path: Path) -> str:
    doc = DocxDocument(str(path))
    paras = [p.text for p in doc.paragraphs if p.text]
    return "\n\n".join(paras)

def read_pptx(path: Path) -> str:
    prs = Presentation(str(path))
    parts: List[str] = []
    for s in prs.slides:
        for shp in s.shapes:
            if hasattr(shp, "text"):
                parts.append(shp.text)
    return "\n\n".join(parts)

def load_text(path: Path) -> str:
    ext = path.suffix.lower()
    if ext == ".pdf":
        return read_pdf(path)
    if ext == ".docx":
        return read_docx(path)
    if ext == ".pptx":
        return read_pptx(path)
    if ext in {".txt", ".md"}:
        return read_txt(path)
    raise HTTPException(400, f"Unsupported file type: {ext}")


def chunk_text(text: str, max_chars=6000, overlap=700) -> List[str]:
    if not text:
        return []
    # header-aware-ish: split on blank lines, then window
    paras = [p.strip() for p in re.split(r"\n\s*\n", text) if p.strip()]
    chunks: List[str] = []
    buf = ""
    for p in paras:
        add = ("\n\n" + p) if buf else p
        if len(buf + add) <= max_chars:
            buf = buf + add
        else:
            if buf:
                chunks.append(buf)
            tail = buf[-overlap:] if overlap and buf else ""
            buf = (tail + "\n\n" + p) if tail else p
    if buf:
        chunks.append(buf)
    # final pass to enforce size
    out: List[str] = []
    for c in chunks:
        if len(c) <= max_chars:
            out.append(c)
        else:
            start = 0
            while start < len(c):
                end = min(start + max_chars, len(c))
                out.append(c[start:end])
                if end == len(c):
                    break
                start = max(0, end - overlap)
    return out

# -------- Pydantic models --------
class IngestResponse(BaseModel):
    file: str
    chunks: int
    inserted: int

class QueryRequest(BaseModel):
    question: str
    course: str = Field(default=COURSE_DEFAULT)
    k: int = 5

class QueryHit(BaseModel):
    id: int
    score: float
    source_uri: str
    preview: str

class QueryResponse(BaseModel):
    hits: List[QueryHit]

class AnswerResponse(BaseModel):
    answer: str
    sources: List[QueryHit]

# -------- DB helpers --------
UPSERT_SQL = """
MERGE INTO doc_chunks d
USING (
  SELECT :course AS course,
         :source_uri AS source_uri,
         :chunk_no AS chunk_no,
         :section AS section,
         :page_from AS page_from,
         :page_to AS page_to,
         :content AS content,
         :embedding AS embedding,
         :metadata AS metadata
  FROM dual
) s
ON (d.course=s.course AND d.source_uri=s.source_uri AND d.chunk_no=s.chunk_no)
WHEN MATCHED THEN UPDATE SET
  d.section=s.section,
  d.page_from=s.page_from,
  d.page_to=s.page_to,
  d.content=s.content,
  d.embedding=s.embedding,
  d.metadata=s.metadata
WHEN NOT MATCHED THEN INSERT (
  course, source_uri, chunk_no, section, page_from, page_to, content, embedding, metadata
) VALUES (
  s.course, s.source_uri, s.chunk_no, s.section, s.page_from, s.page_to, s.content, s.embedding, s.metadata
)
"""


VECTOR_SEARCH_SQL = f"""
SELECT id, source_uri,
       VECTOR_DISTANCE(embedding, :qvec USING COSINE) AS score,
       DBMS_LOB.SUBSTR(content, 300, 1) AS preview
FROM doc_chunks
WHERE course = :course AND embedding IS NOT NULL
ORDER BY score
FETCH FIRST :k ROWS ONLY
"""

# low-tech fallback if no embeddings; not ranked, just first matches
KEYWORD_SEARCH_SQL = """
SELECT id, source_uri,
       9999 AS score,
       DBMS_LOB.SUBSTR(content, 300, 1) AS preview
FROM doc_chunks
WHERE course = :course AND REGEXP_LIKE(content, :pattern, 'i')
FETCH FIRST :k ROWS ONLY
"""

# -------- API --------
@app.get("/health")
def health() -> Dict[str, Any]:
    try:
        pool = get_pool()
        with pool.acquire() as conn:
            with conn.cursor() as cur:
                cur.execute("select 1 from dual")
                _ = cur.fetchone()
        return {"ok": True}
    except Exception as e:
        log.exception("health error")
        return JSONResponse(status_code=500, content={"ok": False, "error": str(e)})


@app.post("/ingest", response_model=IngestResponse)
async def ingest(file: UploadFile = File(...), course: str = Form(default=COURSE_DEFAULT)):
    pool = get_pool()
    # read file to temp
    raw = await file.read()
    tmp = Path("/tmp") / file.filename
    tmp.write_bytes(raw)
    text = load_text(tmp)
    chunks = chunk_text(text)
    vecs = embed(chunks) if chunks else []

    inserted = 0
    with pool.acquire() as conn:
        #ensure_schema(conn)
        with conn.cursor() as cur:
            cur.setinputsizes(
                course=oracledb.DB_TYPE_VARCHAR,
                source_uri=oracledb.DB_TYPE_VARCHAR,
                chunk_no=oracledb.DB_TYPE_NUMBER,
                section=oracledb.DB_TYPE_VARCHAR,
                page_from=oracledb.DB_TYPE_NUMBER,
                page_to=oracledb.DB_TYPE_NUMBER,
                content=oracledb.DB_TYPE_CLOB,
                embedding=oracledb.DB_TYPE_VECTOR,
                metadata=oracledb.DB_TYPE_JSON,
            )
            for i, (c, v) in enumerate(zip(chunks, vecs), start=1):
                cur.execute(UPSERT_SQL, dict(
                    course=course,
                    source_uri=f"upload://{file.filename}",
                    chunk_no=i,
                    section=None,
                    page_from=None,
                    page_to=None,
                    content=c,
                    embedding=[float(x) for x in v],
                    metadata=json.dumps({"file": file.filename}),
                ))
                inserted += 1
        conn.commit()
    return IngestResponse(file=file.filename, chunks=len(chunks), inserted=inserted)


"""@app.post("/query", response_model=QueryResponse)
async def query(req: QueryRequest):
    pool = get_pool()
    with pool.acquire() as conn:
        with conn.cursor() as cur:
            hits: List[QueryHit] = []
            # Try vector first
            try:
                qvec = embed([req.question], is_query=True)[0]
                cur.execute(VECTOR_SEARCH_SQL, {"qvec": qvec, "course": req.course, "k": req.k})
                rows = cur.fetchall()
                for id_, src, score, prev in rows:
                    hits.append(QueryHit(id=id_, source_uri=src, score=float(score), preview=str(prev)))
            except oracledb.Error as e:
                log.warning("vector search failed, falling back: %s", e)

            if not hits:
                # crude keyword fallback
                pattern = re.sub(r"\W+", "|", req.question).strip("|") or req.question
                cur.execute(KEYWORD_SEARCH_SQL, {"course": req.course, "pattern": pattern, "k": req.k})
                rows = cur.fetchall()
                for id_, src, score, prev in rows:
                    hits.append(QueryHit(id=id_, source_uri=src, score=float(score), preview=str(prev)))

    return QueryResponse(hits=hits)"""

@app.post("/query", response_model=QueryResponse)
async def query(req: QueryRequest):
    pool = get_pool()
    with pool.acquire() as conn:
        with conn.cursor() as cur:
            # 1. Embed the query using Cohere
            qvec = embed([req.question], is_query=True)[0]
            qvec = np.array(qvec, dtype=np.float32)

            # 2. Fetch embeddings + metadata from Oracle
            cur.execute("""
                SELECT id, source_uri, embedding, DBMS_LOB.SUBSTR(content, 300, 1) AS preview
                FROM doc_chunks
                WHERE course = :course AND embedding IS NOT NULL
            """, {"course": req.course})

            rows = cur.fetchall()
            if not rows:
                # Fallback: keyword search
                pattern = re.sub(r"\W+", "|", req.question).strip("|") or req.question
                cur.execute(KEYWORD_SEARCH_SQL, {"course": req.course, "pattern": pattern, "k": req.k})
                rows = cur.fetchall()
                hits = [QueryHit(id=id_, source_uri=src, score=float(score), preview=str(prev)) for id_, src, score, prev in rows]
                return QueryResponse(hits=hits)

            # 3. Compute cosine similarity manually
            hits = []
            for id_, src, emb, prev in rows:
                emb = np.array(emb, dtype=np.float32)
                # cosine similarity = dot(a, b) / (||a||*||b||)
                score = float(np.dot(qvec, emb) / (np.linalg.norm(qvec) * np.linalg.norm(emb) + 1e-8))
                hits.append((score, QueryHit(id=id_, source_uri=src, score=score, preview=str(prev))))

            # 4. Sort by similarity (higher is better)
            hits.sort(key=lambda x: x[0], reverse=True)
            top_hits = [hit for _, hit in hits[:req.k]]
            return QueryResponse(hits=top_hits)


"""@app.post("/answer", response_model=AnswerResponse)
async def answer(req: QueryRequest):
    # Retrieve
    qr = await query(req)
    # Simple stub "LLM": extract sentences that seem relevant
    context = "\n\n".join(h.preview for h in qr.hits)
    answer = simple_answer_stub(req.question, context)
    return AnswerResponse(answer=answer, sources=qr.hits)"""

@app.post("/answer", response_model=AnswerResponse)
async def answer(req: QueryRequest):
    # Retrieve top-k context
    qr = await query(req)
    context = "\n\n".join(h.preview for h in qr.hits)

    # Generate an LLM answer using Cohere
    if not context:
        answer = "I couldn’t find relevant course material for that question."
    else:
        answer = generate_answer_with_cohere(req.question, context)

    return AnswerResponse(answer=answer, sources=qr.hits)



# -------- tiny LLM stub --------
def simple_answer_stub(question: str, context: str) -> str:
    # super-naive: return top few sentences from context that share words with question
    qwords = set(w.lower() for w in re.findall(r"\w+", question))
    sents = re.split(r"(?<=[.!?])\s+", context)
    scored = []
    for s in sents:
        sword = set(w.lower() for w in re.findall(r"\w+", s))
        overlap = len(qwords & sword)
        if overlap:
            scored.append((overlap, s))
    scored.sort(reverse=True)
    picked = " ".join(s for _, s in scored[:4])
    if not picked:
        picked = (context[:512] + "…") if context else "No context available."
    return f"Based on the indexed materials, here's what I found:\n\n{picked}"

import cohere

COHERE_API_KEY = os.getenv("COHERE_API_KEY")
co = cohere.Client(COHERE_API_KEY)

def generate_answer_with_cohere(question: str, context: str) -> str:
    """
    Generate a context-aware answer using Cohere's latest Chat API (C4 family).
    """
    message = f"""You are a helpful teaching assistant for a Data Science course.

Use the provided context to answer the student's question clearly and concisely.

---
Context:
{context}

---
Question: {question}
Answer:"""

    # ✅ latest model as of late 2025
    response = co.chat(
        model="command-a-03-2025",
        message=message,
        temperature=0.5,
    )

    return response.text.strip()



# -------- main --------
if __name__ == "__main__":
    import uvicorn
    log.info("Starting RAG service (dsn=%s, course=%s, fake_embed=%s)", ADB_DSN, COURSE_DEFAULT, USE_FAKE_EMBED)
    uvicorn.run(app, host="0.0.0.0", port=int(os.environ.get("PORT", "8000")))
